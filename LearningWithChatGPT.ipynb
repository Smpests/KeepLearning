{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVpT8wO7pQXcTTKGg58vxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Smpests/KeepLearning/blob/master/LearningWithChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 零基础用ChatGPT学机器学习"
      ],
      "metadata": {
        "id": "ysjN3MvUKekz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 首先，我们需要安装TensorFlow。您可以在终端或命令提示符中运行以下命令来安装TensorFlow："
      ],
      "metadata": {
        "id": "2gkaigdDLkPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4u6m-UAL-1c",
        "outputId": "efc8bb36-9cc7-4b93-a525-4c6d89bb0b8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 接下来，我们将定义模型的架构。我们将使用一个简单的循环神经网络（RNN）模型，其中包含一个嵌入层、一个LSTM层和一个密集层。"
      ],
      "metadata": {
        "id": "9UF62nRpMKqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class LanguageModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    定义语言对话模型的架构\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super(LanguageModel, self).__init__(self)\n",
        "        # 定义模型的嵌入层\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # 定义模型的LSTM层\n",
        "        self.lstm = tf.keras.layers.LSTM(rnn_units)\n",
        "        # 定义模型的密集层\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        定义模型的前向传播\n",
        "        \"\"\"\n",
        "        # 将输入数据传递给嵌入层，获取嵌入向量\n",
        "        x = self.embedding(inputs)\n",
        "        # 将嵌入向量传递给LSTM层\n",
        "        x = self.lstm(x)\n",
        "        # 将LSTM层的输出传递给密集层，得到预测的下一个词的概率分布\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "class DataGenerator:\n",
        "    def __init__(self, encoded_text, batch_size, seq_length):\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "        self.total_size = batch_size * (seq_length + 1)\n",
        "        self.encoded_text = encoded_text\n",
        "\n",
        "        # 检查文本的长度是否足够生成一个完整的子序列\n",
        "        if len(self.encoded_text) < self.total_size:\n",
        "            raise ValueError(f\"文本长度不足，至少需要 {self.total_size} 个字符。\")\n",
        "\n",
        "        # 我们将 encoded_text 列表分割成多个子序列，每个子序列的长度为 seq_length+1。\n",
        "        # 其中前 seq_length 个元素作为输入序列，最后一个元素作为输出序列。\n",
        "        self.num_batches = len(encoded_text) // self.total_size\n",
        "        self.x_data = np.zeros((self.num_batches, self.batch_size, self.seq_length), dtype=np.int32)\n",
        "        self.y_data = np.zeros((self.num_batches, self.batch_size), dtype=np.int32)\n",
        "        for i in range(self.num_batches):\n",
        "            start = i * self.total_size\n",
        "            end = start + self.total_size\n",
        "            batch_text = encoded_text[start:end]\n",
        "            for j in range(self.batch_size):\n",
        "                x = batch_text[j * self.seq_length:(j + 1) * self.seq_length]\n",
        "                y = batch_text[(j + 1) * self.seq_length]\n",
        "                self.x_data[i, j, :] = x\n",
        "                self.y_data[i, j] = y\n",
        "\n",
        "        # 保存最后一个子序列，用于生成预测\n",
        "        self.last_x = np.zeros((self.batch_size, self.seq_length), dtype=np.int32)\n",
        "        self.last_x[:] = self.x_data[-1, :, :]\n",
        "        self.last_y = self.y_data[-1, :]\n",
        "\n",
        "    def __len__(self):\n",
        "        # 返回生成器的迭代次数\n",
        "        return self.num_batches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 返回给定索引对应的批次\n",
        "        return self.x_data[idx], self.y_data[idx]\n",
        "\n",
        "    def get_last_batch(self):\n",
        "        # 返回最后一个批次，用于生成预测\n",
        "        return self.last_x, self.last_y"
      ],
      "metadata": {
        "id": "7IPNlua7MWm9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 下一步是训练模型并使用它来生成新的文本。这里我们需要定义一些训练参数，包括学习率、批次大小、序列长度等等。"
      ],
      "metadata": {
        "id": "WDKLnNAdM52T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 反复问ChatGPT依然跑不起来\n",
        "\n",
        "# 定义训练参数\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "epochs = 30\n",
        "\n",
        "# 在实际应用中，训练数据可能非常大，可能需要从文件或数据库中读取。\n",
        "# 这里为了简化示例，我们假设训练数据已经存在，并将其作为一个字符串传递给 DataGenerator 类。\n",
        "\n",
        "text = \"This is a sample text for testing purposes. It has a total length of 12928 characters which is enough to generate a batch of size 128 and sequence length 100. This is just a random string of text that is long enough to meet the requirements. We could use any text of sufficient length for this purpose.\"\n",
        "# 定义数据生成器\n",
        "data_generator = DataGenerator(text, batch_size, seq_length)\n",
        "\n",
        "# 创建模型model = LanguageModel(data_generator.vocab_size, 256, 1024)\n",
        "\n",
        "\n",
        "# 定义优化器和损失函数\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 训练模型\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for i in range(data_generator.num_batches):\n",
        "        x_batch = data_generator.x_batches[i]\n",
        "        y_batch = data_generator.y_batches[i]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # 在前向传播期间记录梯度\n",
        "            logits = model(x_batch)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "            # 计算梯度\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            # 根据梯度更新模型参数\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        epoch_loss += loss\n",
        "    print(f\"Epoch {epoch+1}: Loss={epoch_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "prUtWzMENEBo",
        "outputId": "602c3fb4-01e8-48eb-bb0b-ef3b5914edee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4d71843d1ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is a sample text for testing purposes. It has a total length of 12928 characters which is enough to generate a batch of size 128 and sequence length 100. This is just a random string of text that is long enough to meet the requirements. We could use any text of sufficient length for this purpose.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 定义数据生成器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 创建模型model = LanguageModel(data_generator.vocab_size, 256, 1024)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-402bdb85b2ea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoded_text, batch_size, seq_length)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# 检查文本的长度是否足够生成一个完整的子序列\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"文本长度不足，至少需要 {self.total_size} 个字符。\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# 我们将 encoded_text 列表分割成多个子序列，每个子序列的长度为 seq_length+1。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 文本长度不足，至少需要 12928 个字符。"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 训练过程中，我们按批次提供数据，计算模型输出和损失，并使用反向传播算法更新模型参数。每个周期的损失都会被累加并打印出来。\n",
        "\n",
        "完成训练后，我们可以使用模型生成新的文本："
      ],
      "metadata": {
        "id": "ghDNzwRBNLJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义生成文本的参数\n",
        "num_words = 100\n",
        "start_seed = \"hello\"\n",
        "\n",
        "# 将起始种子转换为数字序列\n",
        "token_list = data_generator.tokenizer.texts_to_sequences([start_seed])[0]\n",
        "# 在开始生成新文本之前，我们需要先通过模型预测下一个词\n",
        "# 然后将其添加到种子中，并移除第一个词以维持种子的长度\n",
        "for _ in range(num_words):\n",
        "    x = np.array(token_list[-seq_length:]).reshape(1, seq_length)\n",
        "    # 使用模型预测下一个词\n",
        "    predicted = model(x).argmax(axis=1)[0]\n",
        "    # 将新的词添加到种子中\n",
        "    token_list.append(predicted)\n",
        "# 将数字序列转换回文本\n",
        "generated_text = data_generator.tokenizer.sequences_to_texts([token_list])[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "PrlM17IdNSo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}